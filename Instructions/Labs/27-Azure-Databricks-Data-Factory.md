---
lab:
  title: Azure Data Factory を使用して Azure Databricks ノートブックを自動化する
  ilt-use: Suggested demo
---

# Azure Data Factory を使用して Azure Databricks ノートブックを自動化する

Azure Databricks でノートブックを使用して、データ ファイルの処理やテーブルへのデータの読み込みなどのデータ エンジニアリング タスクを実行できます。 これらのタスクをデータ エンジニアリング パイプラインの一部として調整する必要がある場合は、Azure Data Factory を使用できます。

この演習の所要時間は約 **40** 分です。

## 開始する前に

管理レベルのアクセス権を持つ [Azure サブスクリプション](https://azure.microsoft.com/free)が必要です。

## Azure リソースをプロビジョニングする

この演習では、スクリプトを使用して、Azure サブスクリプションに新しい Azure Databricks ワークスペースと Azure Data Factory リソースをプロビジョニングします。

1. Web ブラウザーで、`https://portal.azure.com` の [Azure portal](https://portal.azure.com) にサインインします。
2. ページ上部の検索バーの右側にある **[\>_]** ボタンを使用して、Azure portal に新しい Cloud Shell を作成します。メッセージが表示されたら、***PowerShell*** 環境を選んで、ストレージを作成します。 次に示すように、Azure portal の下部にあるペインに、Cloud Shell のコマンド ライン インターフェイスが表示されます。

    ![Azure portal と Cloud Shell のペイン](./images/cloud-shell.png)

    > **注**: 前に *Bash* 環境を使ってクラウド シェルを作成している場合は、そのクラウド シェル ペインの左上にあるドロップダウン メニューを使って、***PowerShell*** に変更します。

3. ペインの上部にある区分線をドラッグして Cloud Shell のサイズを変更したり、ペインの右上にある **&#8212;** 、 **&#9723;** 、**X** アイコンを使用して、ペインを最小化または最大化したり、閉じたりすることができます。 Azure Cloud Shell の使い方について詳しくは、[Azure Cloud Shell のドキュメント](https://docs.microsoft.com/azure/cloud-shell/overview)をご覧ください。

4. PowerShell のペインで、次のコマンドを入力して、リポジトリを複製します。

    ```
    rm -r dp-203 -f
    git clone https://github.com/MicrosoftLearning/dp-203-azure-data-engineer dp-203
    ```

5. リポジトリが複製されたら、次のコマンドを入力してこのラボ用のフォルダーに変更し、そこに含まれている **setup.ps1** スクリプトを実行します。

    ```
    cd dp-203/Allfiles/labs/27
    ./setup.ps1
    ```

6. メッセージが表示された場合は、使用するサブスクリプションを選択します (これは、複数の Azure サブスクリプションへのアクセス権を持っている場合にのみ行います)。

7. スクリプトの完了まで待ちます。通常、約 5 分かかりますが、さらに時間がかかる場合もあります。 待っている間に、「[Azure Data Factory とは何ですか](https://docs.microsoft.com/azure/data-factory/introduction)」を確認してください。
8. スクリプトが完了したら、Cloud Shell ペインを閉じ、スクリプトによって作成された **dp203-*xxxxxxx*** リソース グループを参照して、Azure Databricks ワークスペースと Azure Data Factory (V2) リソースが含まれていることを確認します (リソース グループ ビューを更新することが必要な場合があります)。

## ノートブックをインポートする

Azure Databricks ワークスペースにノートブックを作成して、さまざまなプログラミング言語で記述されたコードを実行できます。 この演習では、Python コードを含む既存のノートブックをインポートします。

1. Azure portal の **dp203-*xxxxxxx*** リソース グループで、Azure Databricks Service リソース **databricks*xxxxxxx*** を選択します。
2. **databricks*xxxxxxx*** の **[概要]** ページで、 **[ワークスペースの起動]** ボタンを使用して、新しいブラウザー タブで Azure Databricks ワークスペースを開きます。求められた場合はサインインします。
3. **現在のデータ プロジェクトの内容**に関するメッセージが表示された場合は、 **[完了]** を選択して閉じます。 次に、Azure Databricks ワークスペース ポータルを表示し、左側のサイド バーに、実行できるさまざまなタスクのアイコンが含まれていることに注意してください。 サイド バーを展開すると、タスク カテゴリの名前が表示されます。
4. サイド バーを展開し、 **[ワークスペース]** タブを選択します。次に、 **[ユーザー]** フォルダーを選択し、 **&#8962; <ユーザー名>** フォルダーに対して **&#9662;** メニューの **[インポート]** を選択します。**
5. **[ノートブックのインポート]** ダイアログ ボックスで、 **[URL]** を選択し、`https://github.com/MicrosoftLearning/dp-203-azure-data-engineer/raw/master/Allfiles/labs/27/Process-Data.dbc` からノートブックをインポートします
6. **[&#8962; ホーム]** を選択してから、先ほどインポートした **Process Data** ノートブックを開きます。

    **注**: ヒントが表示された場合は、 **[了解]** ボタンを使用して閉じます。 これは、ワークスペース インターフェイスで初めて移動したときに表示されることがある、以降のすべてのヒントに適用されます。

7. ノートブックの内容を確認します。次のことを行う Python コード セルが含まれています。
    - **folder** という名前のパラメーターが渡されている場合は、それを取得します (それ以外の場合は既定値の *data* を使用します)。
    - GitHub からデータをダウンロードし、Databricks ファイル システム (DBFS) 内の指定したフォルダーに保存します。
    - ノートブックを終了し、データが保存されたパスを出力として返します

    > **ヒント**: ノートブックには、必要な実質的にあらゆるデータ処理ロジックを含めることができます。 この簡単な例は、主要な原則を示すために設計されています。

## Azure Databricks と Azure Data Factory の統合を有効にする

Azure Data Factory パイプラインから Azure Databricks を使用するには、Azure Databricks ワークスペースへのアクセスを可能にするリンク サービスを Azure Data Factory 内に作成する必要があります。

### アクセス トークンを生成する

1. Azure Databricks ポータルの左上のメニュー バーで、ユーザー名を選択し、ドロップダウンから **[ユーザー設定]** を選択します。
2. **[ユーザー設定]** ページの **[アクセス トークン]** タブで、 **[新しいトークンの生成]** を選択し、*Data Factory* というコメントと空の有効期間 (トークンは有効期限切れにならない) を使用して新しいトークンを生成します。 "トークンが表示されたら、**[完了]*** *を選択する<u>前に</u>コピーする" ように注意してください。
3. コピーしたトークンをテキスト ファイルに貼り付けておくと、この演習の後半で役立ちます。

### Azure Data Factory でリンク サービスを作成する

1. Azure portal に戻り、**dp203-*xxxxxxx*** リソース グループで、Azure Data Factory リソース **adf*xxxxxxx*** を選択します。
2. **[概要]** ページで、 **[スタジオの起動]** を選択して Azure Data Factory Studio を開きます。 メッセージが表示されたらサインインします。
3. Azure Data Factory Studio で、 **[>>]** アイコンを使用して左側のナビゲーション ウィンドウを展開します。 次に、 **[管理]** ページを選択します。
4. **[管理]** ページの **[リンク サービス]** タブで、 **[+ 新規]** を選択して、新しいリンク サービスを追加します。
5. **[新しいリンク サービス]** ペインで、上部にある **[コンピューティング]** タブを選択します。 次に、 **[Azure Databricks]** を選択します。
6. 続けて、次の設定を使用してリンク サービスを作成します。
    - **名前**: AzureDatabricks
    - **説明**: Azure Databricks ワークスペース
    - **統合ランタイム経由で接続する**: AutoResolveInegrationRuntime
    - **アカウントの選択方法**: Azure サブスクリプションから
    - **Azure サブスクリプション**: "サブスクリプションを選択します"**
    - **Databricks ワークスペース**: "**databricksxxxxxxx** ワークスペースを選択します"**
    - **クラスターの選択**: 新しいジョブ クラスター
    - **Databrick ワークスペース URL**: "Databricks ワークスペース URL に自動的に設定されます"**
    - **認証の種類**: アクセス トークン
    - **アクセス トークン**: "アクセス トークンを貼り付けます"**
    - **クラスター バージョン**: 10.4 LTS (Scala 2.12、Spark 3.2.1)
    - **クラスター ノードの種類**: Standard_DS3_v2
    - **Python バージョン**: 3
    - **ワーカー オプション**: 固定
    - **ワーカー数**: 1

## パイプラインを使用して Azure Databricks ノートブックを実行する

リンク サービスを作成し終わったので、それをパイプラインで使用して、前に表示したノートブックを実行できます。

### パイプラインを作成する

1. Azure Data Factory Studio のナビゲーション ウィンドウで、 **[作成]** を選択します。
2. **[作成]** ページの **[ファクトリのリソース]** ペインで、 **[+]** アイコンを使用して**パイプライン**を追加します。
3. 新しいパイプラインの **[プロパティ]** ペインで、名前を **Process Data with Databricks** に変更します。 ツール バーの右端にある **[プロパティ]** ボタン ( **&#128463;<sub>*</sub>** のような外観) を使用して、 **[プロパティ]** ペインを非表示にします。
4. **[アクティビティ]** ペインで **[Databricks]** を展開し、パイプライン デザイナー画面に **[ノートブック]** アクティビティをドラッグします。
5. 新しい **Notebook1** アクティビティが選択された状態で、下部のペインで次のプロパティを設定します。
    - **全般**:
        - **名前**: Process Data
    - **Azure Databricks**:
        - **Databricks のリンク サービス**: "先ほど作成した **AzureDatabricks** リンク サービスを選択します"**
    - **設定**:
        - **ノートブック パス**: "**Users/<ユーザー名>** フォルダーを参照し、**Process Data** ノートブックを選択します"**
        - **基本パラメーター**: "**product_data** の値を持つ **folder** という名前の新しいパラメーターを追加します"**
6. パイプライン デザイナー画面の上にある **[検証]** ボタンを使用して、パイプラインを検証します。 次に、 **[すべて発行]** ボタンを使用して発行 (保存) します。

### パイプラインを実行する

1. パイプライン デザイナー画面の上にある **[トリガーの追加]** を選択し、 **[今すぐトリガー]** を選択します。
2. **[パイプライン実行]** ペインで **[OK]** を選択してパイプラインを実行します。
3. 左側のナビゲーション ウィンドウで、 **[監視]** を選択し、 **[パイプラインの実行]** タブの **Process Data with Databricks** パイプラインを観察します。Spark クラスターを動的に作成してノートブックを実行するため、実行に時間がかかる場合があります。 **[パイプラインの実行]** ページの **[&#8635; 最新の情報に更新]** ボタンを使用して、状態を更新できます。

    > **注**: パイプラインが失敗した場合、Azure Databricks ワークスペースがプロビジョニングされているリージョンで、ジョブ クラスターを作成するためのサブスクリプションのクォータが不足していることがあります。 詳細については、「[CPU コアの制限によってクラスターを作成できない](https://docs.microsoft.com/azure/databricks/kb/clusters/azure-core-limit)」を参照してください。 その場合は、ワークスペースを削除し、別のリージョンに新しいワークスペースを作成してみてください。 次のように、セットアップ スクリプトのパラメーターとしてリージョンを指定できます: `./setup.ps1 eastus`

4. 実行が成功したら、その名前を選択して実行の詳細を表示します。 次に、 **[Process Data with Databricks]** ページの **[アクティビティの実行]** セクションで、**Process Data** アクティビティを選択し、その [出力] アイコンを使用してアクティビティからの出力 JSON を表示します。これは次のようになります。******
    ```json
    {
        "runPageUrl": "https://adb-..../run/...",
        "runOutput": "dbfs:/product_data/products.csv",
        "effectiveIntegrationRuntime": "AutoResolveIntegrationRuntime (East US)",
        "executionDuration": 61,
        "durationInQueue": {
            "integrationRuntimeQueue": 0
        },
        "billingReference": {
            "activityType": "ExternalActivity",
            "billableDuration": [
                {
                    "meterType": "AzureIR",
                    "duration": 0.03333333333333333,
                    "unit": "Hours"
                }
            ]
        }
    }
    ```

5. **runOutput** 値をメモします。これは、ノートブックでデータを保存した *path* 変数です。

## Azure Databricks リソースを削除する

これで Azure Data Factory と Azure Databricks の統合を調べ終わったので、不要な Azure のコストを避け、サブスクリプションの容量を解放するために、作成したリソースを削除する必要があります。

1. Azure Databricks ワークスペースと Azure Data Factory スタジオのブラウザー タブを閉じ、Azure portal に戻ります。
2. Azure portal の **[ホーム]** ページで、**[リソース グループ]** を選択します。
3. Azure Databricks と (管理対象リソース グループではなく) Azure Data Factory ワークスペースが含まれる **dp203-*xxxxxxx*** リソース グループを選択します。
4. リソース グループの **[概要]** ページの上部で、**[リソース グループの削除]** を選択します。
5. リソース グループ名を入力して、削除することを確認し、**[削除]** を選択します。

    数分後に、リソース グループと、それに関連付けられているマネージド ワークスペース リソース グループが削除されます。
