---
lab:
  title: Azure Databricks で SQL ウェアハウスを使用する
  ilt-use: Optional demo
---

# Azure Databricks で SQL ウェアハウスを使用する

SQL は、データのクエリと操作を行うための業界標準の言語です。 多くのデータ アナリストが、SQL を使用してリレーショナル データベース内のテーブルに対してクエリを実行することで、データ分析を実行しています。 Azure Databricks には、データ レイク内のファイルに対してリレーショナル データベース レイヤーを提供する、Spark および Delta Lake テクノロジを基盤に構築された SQL 機能が含まれています。

この演習の所要時間は約 **30** 分です。

## 開始する前に

Azure Databricks SQL ウェアハウスをプロビジョニングするには、少なくとも 1 つのリージョンに管理レベルのアクセス権と十分なクォータを持つ [Azure サブスクリプション](https://azure.microsoft.com/free) が必要です。

## Azure Databricks ワークスペースをプロビジョニングする

この演習では、Premium レベルの Azure Databricks ワークスペースが必要です。

1. Web ブラウザーで、`https://portal.azure.com` の [Azure portal](https://portal.azure.com) にサインインします。
2. ページ上部の検索バーの右側にある **[\>_]** ボタンを使用して、Azure portal に新しい Cloud Shell を作成します。メッセージが表示されたら、***PowerShell*** 環境を選んで、ストレージを作成します。 次に示すように、Azure portal の下部にあるペインに、Cloud Shell のコマンド ライン インターフェイスが表示されます。

    ![Azure portal と Cloud Shell のペイン](./images/cloud-shell.png)

    > **注**: 前に *Bash* 環境を使ってクラウド シェルを作成している場合は、そのクラウド シェル ペインの左上にあるドロップダウン メニューを使って、***PowerShell*** に変更します。

3. ペインの上部にある区分線をドラッグして Cloud Shell のサイズを変更したり、ペインの右上にある **&#8212;** 、 **&#9723;** 、**X** アイコンを使用して、ペインを最小化または最大化したり、閉じたりすることができます。 Azure Cloud Shell の使い方について詳しくは、[Azure Cloud Shell のドキュメント](https://docs.microsoft.com/azure/cloud-shell/overview)をご覧ください。

4. PowerShell のペインで、次のコマンドを入力して、リポジトリを複製します。

    ```
    rm -r dp-203 -f
    git clone https://github.com/MicrosoftLearning/dp-203-azure-data-engineer dp-203
    ```

5. リポジトリが複製されたら、次のコマンドを入力してこのラボ用のフォルダーに変更し、そこに含まれている **setup.ps1** スクリプトを実行します。

    ```
    cd dp-203/Allfiles/labs/26
    ./setup.ps1
    ```

6. メッセージが表示された場合は、使用するサブスクリプションを選択します (これは、複数の Azure サブスクリプションへのアクセス権を持っている場合にのみ行います)。

7. スクリプトの完了まで待ちます。通常、約 5 分かかりますが、さらに時間がかかる場合もあります。 待っている間に、Azure Databricks ドキュメントの記事「[Azure Databricks のデータ ウェアハウスとは](https://learn.microsoft.com/azure/databricks/sql/)」を確認してください。

## SQL ウェアハウスを表示して開始する

1. Azure Databricks ワークスペース リソースがデプロイされたら、Azure portal でそのリソースに移動します。
2. Azure Databricks ワークスペースの **[概要]** ページで、 **[ワークスペースの起動]** ボタンを使用して、新しいブラウザー タブで Azure Databricks ワークスペースを開きます。求められた場合はサインインします。
3. **現在のデータ プロジェクトの内容**に関するメッセージが表示された場合は、 **[完了]** を選択して閉じます。 次に、Azure Databricks ワークスペース ポータルを表示し、左側のサイド バーに、タスク カテゴリの名前が含まれていることを確認します。

    >**ヒント**: Databricks ワークスペース ポータルを使用すると、さまざまなヒントと通知が表示される場合があります。 この演習のタスクを完了するには、これらを無視して、示された指示に従ってください。

1. サイドバーの **[SQL]** で、 **[SQL Warehouses] (SQL Warehouse)** を選択します。
1. ワークスペースに **Starter Warehouse** という名前の SQL ウェアハウスが既に含まれていることを確認します。
1. その SQL ウェアハウスの **[アクション]** ( **&#8285;** ) メニューで、 **[編集]** を選択します。 次に、 **[クラスター サイズ]** プロパティを **[2X-Small]** に設定し、変更を保存します。
1. **[開始]** ボタンを使用して SQL ウェアハウスを起動します (1 - 2 分かかる場合があります)。

> **注**: SQL ウェアハウスの起動に失敗した場合、Azure Databricks ワークスペースがプロビジョニングされているリージョンでサブスクリプションのクォータが不足している可能性があります。 詳細については、「[必要な Azure vCPU クォータ](https://docs.microsoft.com/azure/databricks/sql/admin/sql-endpoints#required-azure-vcpu-quota)」を参照してください。 このような場合は、ウェアハウスの起動に失敗したときのエラー メッセージで詳しく説明されているように、クォータの引き上げを要求してみてください。 または、このワークスペースを削除し、別のリージョンに新しいワークスペースを作成することもできます。 次のように、セットアップ スクリプトのパラメーターとしてリージョンを指定できます: `./setup.ps1 eastus`

## データベース スキーマの作成

1. SQL Warehouse の "実行中" に、サイドバーで **[SQL エディター]** を選択します。**
2. **[スキーマ ブラウザー]** ペインで、*hive_metastore* カタログに **default** という名前のデータベースが含まれていることを確認します。
3. **[新しいクエリ]** ペインで、次の SQL コードを入力します。

    ```sql
    CREATE SCHEMA adventureworks;
    ```

4. **[&#9658; 実行 (1000)]** ボタンを使用して SQL コードを実行します。
5. コードが正常に実行されたら、 **[スキーマ ブラウザー]** ペインで、ペインの下部にある最新の情報に更新ボタンを使用して一覧を更新します。 次に、**hive_metastore** と **adventureworks** を展開して、データベースが作成されたが、テーブルが含まれていないことを確認します。

ここではテーブルに **default** データベースを使用できますが、分析データ ストアを構築する場合は、特定のデータ用のカスタム データベースを作成するのが最適です。

## テーブルを作成する

1. [**products.csv**](https://raw.githubusercontent.com/MicrosoftLearning/dp-203-azure-data-engineer/master/Allfiles/labs/26/data/products.csv) ファイルをローカル コンピューターにダウンロードし、**products.csv** として保存します。
1. Azure Databricks ワークスペース ポータルのサイドバーで、 **[(+) 新規]** を選択し、 **[ファイルのアップロード]** を選択して、コンピューターにダウンロードした **products.csv** ファイルをアップロードします。
1. **[データのアップロード]** ページで **adventureworks** スキーマを選択し、テーブル名を **products** に設定します。 次に、ページの左下隅にある **[テーブルの作成]** を選択します。
1. テーブルが作成されたら、その詳細を確認します。

ファイルからデータをインポートしてテーブルを作成できるため、データベースを簡単に設定できます。 Spark SQL を使用して、コードを使ってテーブルを作成することもできます。 テーブル自体は Hive メタストアのメタデータ定義であり、含まれるデータは Databricks ファイル システム (DBFS) ストレージにデルタ形式で保存されます。

## クエリを作成する

1. サイド バーで **[(+) 新規]** を選択し、 **[クエリ]** を選択します。
2. **[スキーマ ブラウザー]** ウィンドウで、**hive_metastore** と **adventureworks** を展開し、**products** テーブルが一覧に表示されていることを確認します。
3. **[新しいクエリ]** ペインで、次の SQL コードを入力します。

    ```sql
    SELECT ProductID, ProductName, Category
    FROM adventureworks.products; 
    ```

4. **[&#9658; 実行 (1000)]** ボタンを使用して SQL コードを実行します。
5. クエリが完了したら、結果のテーブルを確認します。
6. クエリ エディターの右上にある **[保存]** ボタンを使用して、クエリを **[製品とカテゴリ]** として保存します。

クエリを保存すると、後で同じデータをもう一度簡単に取得できます。

## ダッシュボードを作成する

1. サイド バーで **[(+) 新規]** を選択し、 **[ダッシュボード]** を選択します。
2. **[新しいダッシュボード]** ダイアログ ボックスで、「**Adventure Works Products**」という名前を入力し、 **[保存]** を選択します。
3. **Adventure Works Products** ダッシュボードの **[追加]** ドロップダウン リストで、 **[視覚化]** を選択します。
4. **[視覚化ウィジェットの追加]** ダイアログ ボックスで、 **[製品とカテゴリ]** クエリを選択します。 次に、 **[新しい視覚化の作成]** を選択し、タイトルを **Products Per Category** に設定します。 **[視覚化の作成]** を選択します。
5. 視覚化エディターで、次のプロパティを設定します。
    - **視覚化の種類**: 横棒
    - **横棒グラフ**: 選択済み
    - **Y 列**: カテゴリ
    - **X 列**: 製品 ID : 個数
    - **グループ化**: カテゴリ
    - **凡例の配置**: 自動 (フレキシブル)
    - **凡例項目の順序**: 標準
    - **積み重ね**: スタック
    - **値をパーセンテージに正規化する**: 選択<u>されていません</u>
    - **欠損値と null 値**: Do not display in chart (グラフに表示しない)

6. 視覚化を保存し、ダッシュボードで表示します。
7. **[編集完了]** を選択すると、ユーザーに表示されるとおりのダッシュボードが表示されます。

ダッシュボードは、ビジネス ユーザーとデータ テーブルや視覚化を共有するための優れた方法です。 ダッシュボードを定期的に更新したり、サブスクライバーに電子メールで送信したりするようにスケジュールできます。

## Azure Databricks リソースを削除する

これで Azure Databricks の SQL Warehouse を調べ終わったので、不要な Azure コストを回避し、サブスクリプションの容量を解放するために、作成したリソースを削除する必要があります。

1. Azure Databricks ワークスペースのブラウザー タブを閉じ、Azure portal に戻ります。
2. Azure portal の **[ホーム]** ページで、**[リソース グループ]** を選択します。
3. (管理対象リソース グループではなく) Azure Databricks ワークスペースを含むリソース グループを選択します。
4. リソース グループの **[概要]** ページの上部で、**[リソース グループの削除]** を選択します。
5. リソース グループ名を入力して、削除することを確認し、**[削除]** を選択します。

    数分後に、リソース グループと、それに関連付けられているマネージド ワークスペース リソース グループが削除されます。
