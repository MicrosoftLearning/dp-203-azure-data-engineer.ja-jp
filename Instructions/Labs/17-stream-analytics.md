---
lab:
  title: Azure Stream Analytics の概要
  ilt-use: Suggested demo
---

# Azure Stream Analytics の概要

この演習では、ご自分の Azure サブスクリプションに Azure Stream Analytics ジョブをプロビジョニングし、それを使用してリアルタイム イベント データのストリームのクエリと集計を行い、結果を Azure Storage に格納します。

この演習の所要時間は約 **15** 分です。

## 開始する前に

管理レベルのアクセス権を持つ [Azure サブスクリプション](https://azure.microsoft.com/free)が必要です。

## Azure リソースをプロビジョニングする

この演習では、販売トランザクションのシミュレーション データのストリームをキャプチャして処理し、結果を Azure Storage の BLOB コンテナーに格納します。 ストリーミング データを送信できる Azure Event Hubs 名前空間と、ストリーム処理の結果が格納される Azure Storage アカウントが必要になります。

これらのリソースをプロビジョニングするために、PowerShell スクリプトと ARM テンプレートを組み合わせて使用します。

1. [Azure portal](https://portal.azure.com) (`https://portal.azure.com`) にサインインします。
2. ページ上部の検索バーの右側にある **[\>_]** ボタンを使用して、Azure portal に新しい Cloud Shell を作成します。メッセージが表示された場合は、***PowerShell*** 環境を選択して、ストレージを作成します。 次に示すように、Azure portal の下部にあるペインに、Cloud Shell のコマンド ライン インターフェイスが表示されます。

    ![Azure portal と Cloud Shell のペイン](./images/cloud-shell.png)

    > **注**: 前に *Bash* 環境を使ってクラウド シェルを作成している場合は、そのクラウド シェル ペインの左上にあるドロップダウン メニューを使って、***PowerShell*** に変更します。

3. ペインの上部にある区分線をドラッグして Cloud Shell のサイズを変更したり、ペインの右上にある **&#8212;** 、 **&#9723;** 、**X** アイコンを使用して、ペインを最小化または最大化したり、閉じたりすることができます。 Azure Cloud Shell の使い方について詳しくは、[Azure Cloud Shell のドキュメント](https://docs.microsoft.com/azure/cloud-shell/overview)をご覧ください。

4. PowerShell のペインで、次のコマンドを入力して、この演習を含むリポジトリを複製します。

    ```
    rm -r dp-203 -f
    git clone https://github.com/MicrosoftLearning/dp-203-azure-data-engineer dp-203
    ```

5. リポジトリが複製されたら、次のコマンドを入力してこの演習用のフォルダーに変更し、そこに含まれている **setup.ps1** スクリプトを実行します。

    ```
    cd dp-203/Allfiles/labs/17
    ./setup.ps1
    ```

6. メッセージが表示された場合は、使用するサブスクリプションを選択します (これは、複数の Azure サブスクリプションへのアクセス権を持っている場合にのみ行います)。
7. スクリプトの完了まで待ちます。通常、約 5 分かかりますが、さらに時間がかかる場合もあります。 待っている間に、Azure Stream Analytics ドキュメントの「[Azure Stream Analytics へようこそ](https://learn.microsoft.com/azure/stream-analytics/stream-analytics-introduction)」の記事を確認しましょう。

## ストリーミング データ ソースを表示する

リアルタイム データを処理する Azure Stream Analytics ジョブを作成する前に、クエリを実行する必要があるデータ ストリームを見てみましょう。

1. セットアップ スクリプトの実行が完了したら、クラウド シェル ペインのサイズを変更するか最小化して、Azure portal が表示されるようにします (後でクラウド シェルに戻ります)。 Azure portal で、作成された **dp203-*xxxxxxx*** リソース グループに移動し、このリソース グループに、Azure Storage アカウントと Event Hubs 名前空間が含まれることに注目します。

    リソースがプロビジョニングされた**場所**をメモします。後で、同じ場所に Azure Stream Analytics ジョブを作成します。

2. クラウド シェル ペインで次のコマンドを入力して、100 件のシミュレートされた注文を Azure Event Hubs に送信するクライアント アプリを実行します。

    ```
    node ~/dp-203/Allfiles/labs/17/orderclient
    ```

3. 送信された販売注文データを確認します。各注文は製品 ID と数量で構成されています。 1,000 件の注文を送信した後、アプリが終了します。これには 1 分ほどかかります。

## Azure Stream Analytics ジョブの作成

これで、イベント ハブに到着した販売トランザクション データを処理する Azure Stream Analytics ジョブを作成する準備ができました。

1. Azure portal の **dp203-*xxxxxxx*** ページで、 **[+ 作成]** を選択し、`Stream Analytics job` を検索します。 次に、次のプロパティを使用して **Stream Analytics ジョブ**を作成します。
    - **[基本]** :
        - **サブスクリプション**:お使いの Azure サブスクリプション
        - **リソース グループ**: 既存の **dp203-*xxxxxxx*** リソース グループを選択します。
        - **名前**: `process-orders`
        - **リージョン**: 他の Azure リソースがプロビジョニングされているリージョンを選択します。
        - **ホスティング環境**: クラウド
        - **ストリーミング ユニット**: 1
    - **ストレージ**:
        - **ストレージ アカウントの追加**: オフ
    - **タグ**:
        - *なし*
2. デプロイが完了するのを待ってから、デプロイされた Stream Analytics ジョブ リソースに移動します。

## イベント ストリームの入力を作成する

Azure Stream Analytics ジョブは、販売注文が記録されているイベント ハブから入力データを取得する必要があります。

1. **process-orders** の概要ページで、 **[入力の追加]** を選択します。 次に、 **[入力]** ページで **[ストリーム入力の追加]** メニューを使用して、次のプロパティを含む**イベント ハブ**入力を追加します。
    - **入力エイリアス**: `orders`
    - **サブスクリプションからイベント ハブを選択する:** オン
    - **サブスクリプション**:お使いの Azure サブスクリプション
    - **イベント ハブ名前空間**: **events*xxxxxxx*** Event Hubs 名前空間を選択します
    - **イベント ハブ名**: 既存の **eventhub*xxxxxxx*** イベント ハブを選択します
    - **イベント ハブ コンシューマー グループ**: 既存の **$Default** コンシューマー グループを選択します
    - **認証モード**: システム割り当てマネージド ID を作成します
    - **パーティション キー**: "空白のままにします"**
    - **イベント シリアル化形式**: JSON
    - **[エンコード]**: UTF-8
2. 入力を保存し、作成されるまで待ちます。 いくつかの通知が表示されます。 "**接続テストが成功しました**" という通知を待ちます。

## BLOB ストアの出力を作成する

集計された販売注文データを JSON 形式で Azure Storage BLOB コンテナーに格納します。

1. **process-orders** Stream Analytics ジョブの **[出力]** ページを表示します。 次に **[追加]** メニューを使って、次のプロパティを含む **BLOB Storage/ADLS Gen2** 出力を追加します。
    - **出力エイリアス:** `blobstore`
    - **Select Blob storage/ADLS Gen2 from your subscriptions from your subscriptions (サブスクリプションから BLOB ストレージ/ADLS Gen2 を選択する)** : オン
    - **サブスクリプション**:お使いの Azure サブスクリプション
    - **ストレージ アカウント**: **store*xxxxxxx*** ストレージ アカウントを選択する
    - **コンテナー**: 既存の**data** コンテナーを選択する
    - **認証モード**: マネージド ID: システム割り当て
    - **イベント シリアル化形式**: JSON
    - **形式**: 行区切り
    - **[エンコード]**: UTF-8
    - **書き込みモード**: 結果の到着時に追加する
    - **パス パターン**: `{date}`
    - **日付の形式**: YYYY/MM/DD
    - **時刻の形式**: "適用なし"**
    - **最小行数**: 20
    - **最大時間**: 0 時間、1 分、0 秒
2. 出力を保存し、作成されるまで待ちます。 いくつかの通知が表示されます。 "**接続テストが成功しました**" という通知を待ちます。

## クエリを作成する

Azure Stream Analytics ジョブの入力と出力を定義したので、クエリを使用して入力からデータを選択、フィルター処理、集計し、結果を出力に送信できます。

1. **process-orders** Stream Analytics ジョブの **[クエリ]** ページを表示します。 次に、(イベント ハブに以前に取り込んだ販売注文イベントに基づいた) 入力プレビューが表示されるまでしばらく待ちます。
2. クライアント アプリから送信されたメッセージの **ProductID** と **Quantity** のフィールドと、その他の Event Hubs フィールド (イベントがイベント ハブに追加された日時を示す **EventProcessedUtcTime** フィールドなど) が入力データに含まれていることを確認します。
3. 次のように、既定のクエリを変更します。

    ```
    SELECT
        DateAdd(second,-10,System.TimeStamp) AS StartTime,
        System.TimeStamp AS EndTime,
        ProductID,
        SUM(Quantity) AS Orders
    INTO
        [blobstore]
    FROM
        [orders] TIMESTAMP BY EventProcessedUtcTime
    GROUP BY ProductID, TumblingWindow(second, 10)
    HAVING COUNT(*) > 1
    ```

    このクエリで、**System.Timestamp** (**EventProcessedUtcTime** フィールドに基づく) を使って、各製品 ID の合計数量が計算される各 10 秒の "タンブリング" (重複しないシーケンシャル) 枠の開始と終了が定義されることを確認します。**

4. **[&#9655; クエリのテスト]** ボタンを使用してクエリを検証し、**テスト結果**の状態が **[成功]** であることを保証します (行が返されない場合でも)。
5. クエリを保存します。

## ストリーミング ジョブを実行する

これで、ジョブを実行し、リアルタイムの販売注文データを処理する準備ができました。

1. **process-orders** Stream Analytics ジョブの **[概要]** ページを表示し、 **[プロパティ]** タブでジョブの **[入力]** 、 **[クエリ]** 、 **[出力]** 、 **[関数]** を確認します。 **[入力]** と **[出力]** の数が 0 の場合は、 **[概要]** ページの **[&#8635; 更新]** ボタンを使って、**orders** 入力と **blobstore** 出力を表示します。
2. **[&#9655; 開始]** ボタンを選び、ストリーミング ジョブを今すぐ開始します。 ストリーミング ジョブが正常に開始されたことを通知されるまで待ちます。
3. クラウド シェル ペインを再度開き、必要に応じて再接続してから、次のコマンドを再実行して、さらに 1,000 件の注文を送信します。

    ```
    node ~/dp-203/Allfiles/labs/17/orderclient
    ```

4. アプリが実行されている間に、Azure portal で **dp203-*xxxxxxx*** リソース グループのページに戻り、**store*xxxxxxxxxxxx*** ストレージ アカウントを選択します。
6. ストレージ アカウント ブレードの左側のペインで、**[コンテナー]** タブを選びます。
7. **data** コンテナーを開き、今年の名前のフォルダーが表示されるまで、 **[&#8635; 更新]** ボタンを使用してビューを更新します。
8. **data** コンテナーで、今年のフォルダーと月と日のサブフォルダーが含まれるフォルダー階層内を移動します。
9. 時のフォルダーで、作成されたファイルをメモしておきます。このファイルには、**0_xxxxxxxxxxxxxxxx.json** のような名前が付いています。
10. ファイルの **[...]** メニュー (ファイルの詳細の右側) で **[表示/編集]** を選択し、ファイルの内容を確認します。次のように、10 秒ごとの JSON レコードで構成され、製品 ID ごとに処理された注文数を示します。

    ```
    {"StartTime":"2022-11-23T18:16:25.0000000Z","EndTime":"2022-11-23T18:16:35.0000000Z","ProductID":6,"Orders":13.0}
    {"StartTime":"2022-11-23T18:16:25.0000000Z","EndTime":"2022-11-23T18:16:35.0000000Z","ProductID":8,"Orders":15.0}
    {"StartTime":"2022-11-23T18:16:25.0000000Z","EndTime":"2022-11-23T18:16:35.0000000Z","ProductID":5,"Orders":15.0}
    {"StartTime":"2022-11-23T18:16:25.0000000Z","EndTime":"2022-11-23T18:16:35.0000000Z","ProductID":1,"Orders":16.0}
    {"StartTime":"2022-11-23T18:16:25.0000000Z","EndTime":"2022-11-23T18:16:35.0000000Z","ProductID":3,"Orders":10.0}
    {"StartTime":"2022-11-23T18:16:25.0000000Z","EndTime":"2022-11-23T18:16:35.0000000Z","ProductID":2,"Orders":25.0}
    {"StartTime":"2022-11-23T18:16:25.0000000Z","EndTime":"2022-11-23T18:16:35.0000000Z","ProductID":7,"Orders":13.0}
    {"StartTime":"2022-11-23T18:16:25.0000000Z","EndTime":"2022-11-23T18:16:35.0000000Z","ProductID":4,"Orders":12.0}
    {"StartTime":"2022-11-23T18:16:25.0000000Z","EndTime":"2022-11-23T18:16:35.0000000Z","ProductID":10,"Orders":19.0}
    {"StartTime":"2022-11-23T18:16:25.0000000Z","EndTime":"2022-11-23T18:16:35.0000000Z","ProductID":9,"Orders":8.0}
    {"StartTime":"2022-11-23T18:16:35.0000000Z","EndTime":"2022-11-23T18:16:45.0000000Z","ProductID":6,"Orders":41.0}
    {"StartTime":"2022-11-23T18:16:35.0000000Z","EndTime":"2022-11-23T18:16:45.0000000Z","ProductID":8,"Orders":29.0}
    ...
    ```

11. [Azure Cloud Shell] ペインで、注文クライアント アプリが完了するまで待ちます。
12. Azure portal で、ファイルを最新の情報に更新して、生成された結果の完全なセットを表示します。
13. **dp203-*xxxxxxx*** リソース グループに戻り、**process-orders** Stream Analytics ジョブを再び開きます。
14. Stream Analytics ジョブのページの上部にある **[&#11036; 停止]** ボタンを使ってジョブを停止し、メッセージが表示されたら確認します。

## Azure リソースを削除する

Azure Stream Analytics を調べ終わったら、不要な Azure コストを避けるために、作成したリソースを削除する必要があります。

1. Azure portal の **[ホーム]** ページで、**[リソース グループ]** を選択します。
2. Azure Storage、Event Hubs、Stream Analytics リソースが含まれる **dp203-*xxxxxxx*** リソース グループを選択します。
3. リソース グループの **[概要]** ページの上部で、**[リソース グループの削除]** を選択します。
4. リソース グループ名として「**dp203-*xxxxxxx***」と入力し、これが削除対象であることを確認したら、 **[削除]** を選択します。

    数分後、この演習で作成されたリソースは削除されます。
